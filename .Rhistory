#          Wie viele Vorhersagen enthalten im 50%-Intervall den Echtpreis?
#          Wie groß (=US$) ist das 50%-Intervall der Vorhersagen im Schnitt?
#          Hat das Modell also eine gute Vorhersagekraft?
#
#          Tipp: ein Dataframe mit allen Daten Übergeben, aber zwei Likelihood-Schleifen:
#          einmal über die "Echtdaten" (750 Datensätze) und einmal über die Testdaten (143 Datensätze).
#          Auf passenden Array-Index aufpassen!
modell1 = "
data {
N <- length(price.log[])
Nlocation <- max(location)
Ndecade <- max(decade)
}
model {
for (i in 1:750) {
price.log[i] ~ dnorm(mu[i], 1/sigma[location[i]]^2)  #!!Grup-pe1  ACHTUNG: hier könnte sein das die Gruppe 2 genommen werden muss jenachdem wir man oben die Gruppe mit der Varianz definiert
mu[i] <- interceptdecade[decade[i]] +           		# Gruppe fuer decade / intercept
interceptlocation[location[i]] +                        # Gruppe fuer location /intercept
beta.bedrooms*bedrooms[i] +
beta.bathrooms*bathrooms[i] +
beta.garage*garage[i] +
beta.area*area[i]
#----------------------------Beginn von Vorhersage-----------------------------------#
#price.log.hat[i] ~ dnorm(mu[i], 1/sigma[location[i]]^2) #ist gleich erste Zeile im Modell
#-----------------------------Ende der Vorhersage------------------------------------#
}
for (i in 751:893) {
price.log.hat[i-750] ~ dnorm(mu[i], 1/sigma[location[i]]^2)  #!!Grup-pe1  ACHTUNG: hier könnte sein das die Gruppe 2 genommen werden muss jenachdem wir man oben die Gruppe mit der Varianz definiert
mu[i] <- interceptdecade[decade[i]] +           		# Gruppe fuer decade / intercept
interceptlocation[location[i]] +                        # Gruppe fuer location /intercept
beta.bedrooms*bedrooms[i] +
beta.bathrooms*bathrooms[i] +
beta.garage*garage[i] +
beta.area*area[i]
}
#-----------------------------------Priors-------------------------------------------#
for(l in 1:Nlocation){
sigma[l]~dexp(3/1)
}
#--------------------------ACHTUNG Partial Pooling Gruppe 1-----------------------------------#
#interceptlocation[l]~dnorm(0,1)
#sigma[l]~dexp(3/1)
#wenn in der Fragestellung nach der Gruppe gefragt wird die Pooling verlangt, dann gehört
#diese Funktion in die forschleife des Partial-Poolings
#------------------------------------------------------------------------------------#
beta.bedrooms ~ dnorm(0,1/1^2)
beta.bathrooms ~ dnorm(0,1/1^2)
beta.garage ~ dnorm(0,1/1^2)
beta.area ~ dnorm(0,1/1^2)
for(l in 1:Nlocation)
{
interceptlocation[l] ~ dnorm(0, 1/1^2)
}
# location hat nur 3 Werte: kein Pooling
# alle locations bekommen das selbe sgima & intercept
#VT: wenn 7 locations wären, dann muesste man sonst 7 Zeilen fuer intercept und 7 fuer sigma schreiben
#--------------------------------Ende von Priors-------------------------------------#
#-------------------------Partial-Pooling fuer decade-------------------------------#
interceptdecade.mu ~ dnorm(0,1/1^2)  #entfernen wenn kein partial pooling
interceptdecade.sigma ~ dexp(1)      #entfernen wenn kein partial pooling
for (d in 1:Ndecade) {
interceptdecade[d] ~ dnorm(interceptdecade.mu, 1/interceptdecade.sigma^2) #wenn kein Partial Pooling dann nach: .mu, 1 #/....
}
#--------stabilere Faktoren ausrechnen (Gruppen beeinflussen sich gegeNdecadeeitig)
for (l in 1:Nlocation) {
for (d in 1:Ndecade) {
mtx[l,d] <- interceptlocation[l] +interceptdecade[d]
}
}
intercept <- mean(mtx[1:Nlocation,1:Ndecade])
for(l in 1:Nlocation)
{
alphalocation[l] <- mean(mtx[l,1:Ndecade]) - intercept
}
for (d in 1:Ndecade)
{
Gammadecade[d] <- mean(mtx[1:Nlocation,d]) - intercept  # mtx ist definiert mit mtx[l,d]
}
#------------------------Ende Partial-Pooling fuer decade-----------------------------#
}
"
#Modell fuer Variablen aufrufen
modell.fit1 = run.jags(model=modell1,
data=dfz,
burnin = 5000,
monitor = c("intercept", "alphalocation", "Gammadecade", "sigma",
"beta.bedrooms", "beta.bathrooms", "beta.garage", "beta.area",
"price.log.hat"),
n.chains = 3,
sample= 10000,
thin=2,
inits = list(list(.RNG.name="base::Mersenne-Twister", .RNG.seed=456),
list(.RNG.name="base::Super-Duper", .RNG.seed=123),
list(.RNG.name="base::Wichmann-Hill", .RNG.seed=789)),
method = "parallel")
fit.samples = as.matrix(modell.fit$mcmc)
fit2.samples = as.matrix(modell.fit$mcmc)
fit2.summary = view(modell.fit)
fit2.samples = as.matrix(modell.fit2$mcmc)
fit1.samples = as.matrix(modell.fit1$mcmc)
fit1.summary = view(modell.fit1)
max(fit1.summary[,"MC%ofSD"])
sum(fit1.summary[,"SSeff"]<=((modell.fit1$sample*3)/100)*10)
fit1.summary
diagMCMC(modell.fit2$mcmc,"price.log.hat[1]")
diagMCMC(modell.fit1$mcmc,"price.log.hat[1]")
max(fit1.summary[,"MC%ofSD"])
fit1.samples
#          Wie viele Vorhersagen enthalten im 50%-Intervall den Echtpreis?
dim(fit1.samples)
fit1.summary.hat = fit1.summary[, -(1:21)]
fit1.summary.hat
fit1.summary.hat = fit1.summary.hat[, -(1:23)]
fit1.summary.hat
fit1.summary.hat = fit1.samples[, -(1:23)]
fit1.summary.hat
dfz$price.log
#          Welchen Einfluss haben die verschiedenen Variablen auf den Preis?
#          (z.B. um wie viel steigt der Preis, wenn ich 1 Schlafzimmer mehr haben will?)
plotcoef(modell.fit, "beta")
library(car)
library(corrplot)
library(runjags)
library(akit2)
# Daten von EU-Wahl 2014
w2014 = read.csv('C:\\Users\\Dominik\\Downloads\\Endgueltiges_Ergebnis_mit_Briefwahl_2014.csv', stringsAsFactors = F)
w2014 = w2014[,c(1:3,5:8,10,12,14,16,18,20,22,24)] # ohne Prozentspalten
colnames(w2014)[3:11] = c('Wahlberechtigte', 'abgegeben', 'ungueltig', 'gueltig',
'OEVP', 'SPOE', 'FPOE', 'GRUENE', 'BZOE')
w2014 = within(w2014, {
abgegeben = as.integer(abgegeben)
ungueltig = as.integer(ungueltig)
gueltig = as.integer(gueltig)
nicht.waehler = Wahlberechtigte - abgegeben
kleine = BZOE + REKOS + ANDERS + EUSTOP
})
# kleine Parteien weg
w2014 = w2014[,c(-11,-13,-14,-15)]
omit2014 = c(1, # Kommentarzeile
grep('^G.0000', w2014$GKZ),  # BundeslÃ¤nder
grep('^G.[A-Z]', w2014$GKZ), # Bezirke
grep('^G...99', w2014$GKZ),  # Wahlkarten (sind in Summen der StÃ¤dte enthalten)
grep('^G9..01', w2014$GKZ),  # Wien ohne Wahlkarten
grep('^G[1-7]0101', w2014$GKZ), # HauptstÃ¤dte ohne Wahlkarten
grep('G10201|G30301|G40201|G30401|G40301|G20201', w2014$GKZ) # letzte Ãberbleibsel
)
at2014 = w2014[2,]
w2014 = w2014[-omit2014,] # ohne Summenzeilen
# Daten von EU-Wahl 2019
w2019 = read.csv('C:\\Users\\Dominik\\Downloads\\Vorlaeufiges_Ergebnis_mit_Briefwahl_2019.csv', stringsAsFactors = F)
w2019 = w2019[,c(1:7,9,11,13,15,17,19)] # ohne Prozentspalten
colnames(w2019)[3:12] = c('Wahlberechtigte', 'abgegeben', 'ungueltig', 'gueltig',
'OEVP', 'SPOE', 'FPOE', 'GRUENE', 'NEOS', 'KPOE')
w2019 = within(w2019, {
abgegeben = as.integer(abgegeben)
ungueltig = as.integer(ungueltig)
gueltig = as.integer(gueltig)
nicht.waehler = Wahlberechtigte - abgegeben
kleine = KPOE + EUROPA
})
# kleine Parteien weg
w2019 = w2019[,c(-12,-13)]
omit2019 = c(1, # Kommentarzeile
grep('^G.0000', w2019$GKZ),  # BundeslÃ¤nder
grep('^G.[A-Z]', w2019$GKZ), # Bezirke
grep('^G...99', w2019$GKZ),  # Wahlkarten (sind in Summen der StÃ¤dte enthalten)
grep('^G9..01', w2019$GKZ),  # Wien ohne Wahlkarten
grep('^G[1-7]0101', w2019$GKZ), # HauptstÃ¤dte ohne Wahlkarten
grep('G10201|G30301|G40201|G30401|G40301|G20201', w2019$GKZ) # letzte Ãberbleibsel
)
at2019 = w2019[2,]
w2019 = w2019[-omit2019,] # ohne Summenzeilen
# JOIN
wahlen = merge(w2014, w2019, by='GKZ', suffixes=c('14','19'))
# Steiermark
stmk = wahlen[grep('^G6', wahlen$GKZ),]
stmk$groesse = factor(ifelse(stmk$Wahlberechtigte14 < 1200, 'klein',
ifelse(stmk$Wahlberechtigte14 < 5000, 'mittel', 'gross')))
# Aufteilen in 2 Gruppen
set.seed(12)
bekannt = sample.int(nrow(stmk), 100)
# JAGS vertrÃ¤gt keine Variablen, die Zeichenketten sind
data.columns = grep('^([^G]|GR)', colnames(stmk))
stmk.bek = stmk[bekannt, data.columns]
stmk.unbek = stmk[-bekannt, data.columns]
stmk.bek.name = stmk$Gebietsname19[bekannt]
stmk.unbek.name = stmk$Gebietsname19[-bekannt]
# Erstes Modell
modell1 = "
data {
N <- length(Wahlberechtigte14[])
sdOEVP = sd(OEVP14)
sdSPOE = sd(SPOE14)
sdFPOE = sd(FPOE14)
sdGRUENE = sd(GRUENE14)
sdNEOS = sd(NEOS14)
sdKleine = sd(kleine14)
sdNW = sd(nicht.waehler14)
sdOEVP19 = sd(OEVP19)
maxOEVP19 = max(OEVP19)
}
model {
for (i in 1:N) {
OEVP19[i] ~ dnorm(mu[i], 1/sigma^2)
mu[i] <- intercept +
beta.oevp*OEVP14[i] +
beta.spoe*SPOE14[i] +
beta.fpoe*FPOE14[i] +
beta.gruene*GRUENE14[i] +
beta.neos*NEOS14[i] +
beta.kleine*kleine14[i] +
beta.nw*nicht.waehler14[i]
}
sigma ~ dexp(3/(2*sdOEVP19))
intercept ~ dnorm(0, 1/(3*maxOEVP19)^2)
beta.oevp ~ dnorm(0, 1/(2*sdOEVP19/sdOEVP)^2)
beta.spoe ~ dnorm(0, 1/(2*sdOEVP19/sdSPOE)^2)
beta.fpoe ~ dnorm(0, 1/(2*sdOEVP19/sdFPOE)^2)
beta.gruene ~ dnorm(0, 1/(2*sdOEVP19/sdGRUENE)^2)
beta.neos ~ dnorm(0, 1/(2*sdOEVP19/sdNEOS)^2)
beta.kleine ~ dnorm(0, 1/(2*sdOEVP19/sdKleine)^2)
beta.nw ~ dnorm(0, 1/(2*sdOEVP19/sdNW)^2)
}
"
m1 = run.jags(modell1,
monitor=c('intercept', 'sigma',
'beta.oevp', 'beta.spoe', 'beta.fpoe', 'beta.gruene',
'beta.neos', 'beta.kleine', 'beta.nw'),
data=stmk.bek,
n.chains = 3)
view(m1)
diagMCMC(m1$mcmc, 'beta.neos')
#z-transformieren und gruppenvariablen in integer verwanden - ALL IN ONE Befehl :)
stmk.bek.z = prepare.df.bayes(stmk.bek, drop.originals = TRUE)
m1z = run.jags(modell1,
monitor=c('intercept', 'sigma',
'beta.oevp', 'beta.spoe', 'beta.fpoe', 'beta.gruene',
'beta.neos', 'beta.kleine', 'beta.nw'),
data=stmk.bek.z,
n.chains = 3)
view(m1z)
#Schaut immer noch nicht gut aus!
stmk.bek.lz = zscale.df(log2(stmk.bek[, -23] + 1), drop.originals = TRUE)
m1lz = run.jags(modell1,
monitor=c('intercept', 'sigma',
'beta.oevp', 'beta.spoe', 'beta.fpoe', 'beta.gruene',
'beta.neos', 'beta.kleine', 'beta.nw'),
data=stmk.bek.lz,
n.chains = 3)
view(m1lz)
diagMCMC(m1lz$mcmc, 'beta.neos')
# Zweites Modell
modell2 = "
model {
for (i in 1:100) {
OEVP19[i] ~ dnorm(mu[i], 1/sigma^2)
mu[i] <- beta.oevp*OEVP14[i] +
beta.spoe*SPOE14[i] +
beta.fpoe*FPOE14[i] +
beta.gruene*GRUENE14[i] +
beta.neos*NEOS14[i] +
beta.kleine*kleine14[i] +
beta.nw*nicht.waehler14[i]
}
for (i in 101:171) {
OEVP19.hat[i-100] ~ dnorm(mu[i], 1/sigma^2)
mu[i] <- beta.oevp*OEVP14[i] +
beta.spoe*SPOE14[i] +
beta.fpoe*FPOE14[i] +
beta.gruene*GRUENE14[i] +
beta.neos*NEOS14[i] +
beta.kleine*kleine14[i] +
beta.nw*nicht.waehler14[i]
}
sigma ~ dexp(3/2)
beta.oevp ~ dnorm(0, 1/2^2)
beta.spoe ~ dnorm(0, 1/2^2)
beta.fpoe ~ dnorm(0, 1/2^2)
beta.gruene ~ dnorm(0, 1/2^2)
beta.neos ~ dnorm(0, 1/2^2)
beta.kleine ~ dnorm(0, 1/2^2)
beta.nw ~ dnorm(0, 1/2^2)
}
"
#Neuer Befehl um Daten zu transformieren wie einen anderen Datensatz!
stmk.unbek.lz = zscale.df.other(log2(stmk.unbek[, -23]+1), stmk.bek.lz, drop.originals = TRUE)
m2lz = run.jags(modell2,
monitor=c('sigma',
'beta.oevp', 'beta.spoe', 'beta.fpoe', 'beta.gruene',
'beta.neos', 'beta.kleine', 'beta.nw', 'OEVP19.hat'),
data=rbind(stmk.bek.lz, stmk.unbek.lz),
n.chains = 3)
samples2 = as.matrix(m2lz$mcmc)
dim(samples2)
stmk.samples = samples2[, -(1:8)]
#Löschen sigma, beta.oevp, etc. raus weil wir nur Gemeinden (OEVP19.hat) haben wollen
gemeinde = 37
gem.samples = stmk.samples[, gemeinde]
stmk.unbek.name[gemeinde]
#Zurücktransformieren und es muss der Datensatz genommen werden den wir zum Umrechnen
#schon rein gegeben haben.
gem.orig = inv.zscale.other(gem.samples, stmk.bek.lz$OEVP19)
gem.orig = 2^gem.orig - 1
plotPost(gem.orig, compVal = stmk.unbek$OEVP19[gemeinde])
#Wir sagen 598 vorraus und in wirklichkeit waren 535.
gem.samples
stmk.samples[, gemeinde]
stmk.samples
stmk.samples
stmk.unbek.name
gemeinde
stmk.unbek.name[gemeinde]
gem.samples
stmk.samples
gem.samples
stmk.unbek.name[gemeinde]
#Zurücktransformieren und es muss der Datensatz genommen werden den wir zum Umrechnen
#schon rein gegeben haben.
gem.orig = inv.zscale.other(gem.samples, stmk.bek.lz$OEVP19)
gem.orig
gem.samples
stmk.bek.lz$OEVP19
stmk.bek.lz
gem.orig
gem.orig
gem.orig = 2^gem.orig - 1
gem.orig
plotPost(gem.orig, compVal = stmk.unbek$OEVP19[gemeinde])
plotPost(gem.orig, compVal = stmk.unbek$OEVP19[gemeinde])
library(ggplot2)
library(car)
library(corrplot)
library(effects)
library(pwr)
library(ROCR)
library(runjags)
library(coda)
rjags::load.module("glm")
library(akit2)
df <- read.csv('C:\\Users\\Dominik\\Downloads\\deformation.csv')
# Referenz-Gruppe Ã¤ndern
df$material = relevel(df$material, ref = 'holz')
view(df)
hist(df$durchsatz)
#Ist aber Normalverteilt. Gut.
hist(df$deform)
hist(df$masse)
hist(df$geschw)
plot(df$deform~df$masse)
plot(df$deform~log2(df$masse))
#Sieht log-transformiert schon Besser aus.
hist(log2(df$masse))
#            und Geschwindigkeit mit auf.
#            Berechnen Sie getrennt für beide Modelle, wie viele DatensÃ¤tze wir
#            mindestens benätigen, um einen Modelleffekt mit fÂ²=0.05 mit einer
#            Wahrscheinlichkeit von 95% zu erkennen.
#
#            Nehmen Sie dann mittels df[1:___,] nur so viele EintrÃ¤ge aus dem Datensatz,
#            wie die grÃ¶Ãere der beiden Power-Analysen ergibt.
#
#            Falls Sie diesen Schritt auslassen wollen:
#            Verwenden Sie 450 DatensÃ¤tze.
model1=lm(deform~masse+material+geschw+durchsatz,data=df)
model2=lm(deform~masse+material+geschw+durchsatz+masse:geschw,data=df)
qqp(model1)
outlierTest(model1)
plot(model1)
plot(model1)
power=pwr.f2.test(u= 7, v=1000-2, f2=,power= 0.95)
power
power=pwr.f2.test(u= 7, v=1000-2, f2=0.05 ,power= 0.95)
power=pwr.f2.test(u= 7, f2=0.05 ,power= 0.95)
power
power=pwr.f2.test(u= 6, f2=0.05 ,power= 0.95)
power
n = round(power$v+6+1)
n
power2=pwr.f2.test(u= 7, f2=0.05 ,power= 0.95)
n2 = round(power2$v+7+1)
n2
dfn=df[1:445,]
model1=lm(deform~masse+material+geschw+durchsatz,data=dfn)
model2=lm(deform~masse+material+geschw+durchsatz+masse:geschw,data=dfn)
#------------------------------------------------Kolinearität
#(Prüfung der Beeinflussung der Variablen untereinander)
#Kleiner 3 ist super. Ab Wert 3 sollten wir cor() anschauen!
vif(model1)
#------------------------------------------------Homogene Varianz
#Die Variablen prüfen ob ein Muster vorhanden ist z.B Trichter
par(mfrow=c(2,2))
plot(model)
plot(model1)
#------------------------------------------Normalverteilte Residuen
#kann uns die Verteilung der Residuels zeigen bedeutet aber nicht das wir das immer machen müssen
hist(model1$residuals)
#------------------------------------------Normalverteilte Residuen
#kann uns die Verteilung der Residuels zeigen bedeutet aber nicht das wir das immer machen müssen
hist(model1$residuals)
qqp(model1)
#--------------------------------------------Lineare Abhängigkeit
#crPlots eignen sich, um herauszufinden, ob eine Variable nicht-linear mit y zusammenhängt.
crPlots(model1)
########################### Ausreißer Tests ###############################
outlierTest(model1)
plot(model1, which=4)
#Alles unter 0.5. Sieht gut aus.
plot(model1, which=5)
m1.aov = Anova(model1, type=2) #--> type3 würde den Intercept mitanzeigen
ssq = m1.aov[,1]
names(ssq) = rownames(m1.aov) #Ergebnis passende Namen geben
ssq / m1.aov["Residuals",1]
drop1(model1) #AIC interpretiern und größer ist besser
drop1(model1)[order(drop1(model1)[,4]),]
summary(model1)
4.348e-01
#Bei erhöhung der Masse um 1mg steigt die Deformation um 0.4348
8.863e-04
#Bei erhöhung der Masse um 1mg steigt die Deformation um 0.4348
3.323e+00
2.886e+02
#------------------------------------------------Kolinearität
vif(model2)
#----------------------------------------------Homogene Varianz
par(mfrow=c(2,2))
plot(model2)
#------------------------------------------Normalverteilte Residuen
hist(model2$residuals)
#------------------------------------------Normalverteilte Residuen
hist(model2$residuals)
qqp(model2)
########################### Ausreißer Tests ###############################
outlierTest(model2)
plot(model2, which=4)
#Alles unter 0.5. Sieht gut aus.
plot(model2, which=5)
m1.aov = Anova(model2, type=2) #--> type3 würde den Intercept mitanzeigen
ssq = m1.aov[,1]
names(ssq) = rownames(m1.aov) #Ergebnis passende Namen geben
ssq / m1.aov["Residuals",1]
drop1(model2) #AIC interpretiern und größer ist besser
drop1(model2)[order(drop1(model1)[,4]),]
drop1(model2)[order(drop1(model2)[,4]),]
drop1(model2) #AIC interpretiern und größer ist besser
Anova(model2, type=3)
m1.aov = Anova(model2, type=3) #--> type3 würde den Intercept mitanzeigen
ssq = m1.aov[,1]
names(ssq) = rownames(m1.aov) #Ergebnis passende Namen geben
ssq / m1.aov["Residuals",1]
#######_Berechnen der Bedeutsamkeit der Variablen innerhalb meines Modells_########
Anova(model2, type = 3)
drop1(model2)[order(drop1(model2)[,4]),]
#masse und geschw korrelieren wohl mit dem Interaktionsterm.
#Andere Werte schauen ok aus.
cor(dfn$masse, dfn$masse*dfn$geschw)
cor(dfn$geschw, dfn$geschw*dfn$masse) #Korreliert stark mit 0.83
######################## Interpretation ##########################
# Der Datensatz enthält:
# - durchsatz ... wie viel Material (Volumen) gesiebt wird (Kubikdezimeter pro Stunde, dm^3/h)
# - masse ... Masse der einzelnen Teile (mg)
# - geschwindigkeit ... Geschwindigkeit mit der Teile auf Sieb treffen (m/s)
# - material ... Art der Teile
# - deformation ... Maß für entstandene Deformation des Siebs
summary(model2)
3.001e+02
confint(model2)
######################## Interpretation ##########################
# Der Datensatz enthält:
# - durchsatz ... wie viel Material (Volumen) gesiebt wird (Kubikdezimeter pro Stunde, dm^3/h)
# - masse ... Masse der einzelnen Teile (mg)
# - geschwindigkeit ... Geschwindigkeit mit der Teile auf Sieb treffen (m/s)
# - material ... Art der Teile
# - deformation ... Maß für entstandene Deformation des Siebs
summary(model2)
#Der Basisfall ist mit Material Holz und alle anderen Werte sind auf 0 (nicht sinnvoll). Hier
#liegt der Deformationswert bei 300.1.
8.655e-04
#Bei erhöhung des Durchsatzes um 1dm^3/h steigt die Deformation um 0.0008655
4.089e+00
#-------------------------------------------Cohens f2
#dazu müssen wir aus beiden Modellen die r.squared herrausholen
r2.m1 = summary(model1)$r.squared
r2.m1b = summary(model2)$r.squared
cohensF2=(r2.m1 - r2.m1b) / (1 - r2.m1)
cohensF2
######################## Modelle vergleichen ##########################
summary(model1)
#Adjusted R-squared:  0.5318
summary(model2)
#--------------------------------------------anova
#Mit dem kleingeschriebenen anova können wir mehrere Modelle miteinander vergleichen
anova(model1, model2)
#Adjusted R-squared:  0.5466
#Interaktionsterm ist signifikant.
summary(model1)$adj.r.squared
summary(model2)$adj.r.squared
#---------------------------------------------AIC
AIC(model1, model2)
########################### Vorhersagen ##################################
#----------------Vorhersagegenauigkeit berechnen
#vorhersage des kompletten Models mit den jeweiligen Testdaten
dfp=df[901:1000,]
p1 = predict(model1, dfp)
p2 = predict(model2, dfp)
#echte Daten -vorhersage daten rechnen quadrieren wegen sum of squares
ssq1 = sum((dfp$y - p1)^2)
ssq2 = sum((dfp$y - p2)^2)
c(ssq1, ssq2)
p1
#echte Daten -vorhersage daten rechnen quadrieren wegen sum of squares
ssq1 = sum((dfp$deform - p1)^2)
ssq2 = sum((dfp$deform - p2)^2)
c(ssq1, ssq2)
